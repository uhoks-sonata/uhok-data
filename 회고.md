### 잘한점 : 
**1. 자동화 및 모듈화**

- Apache Airflow의 활용을 통해 데이터 ETL 파이프라인 자동화를 구축했고 DAG를 구성함으로 작업 순서와 병렬 처리를 도입했습니다.
- 여러 다른 웹페이지의 데이터를 각 데이터의 형태에 맞게 수집하는 과정에서 수집하지 못하거나 잘못된 형태로 수집되는 데이터에 대해 예외 처리 코드를 작성하며 각종 데이터를 안정적으로 수집할 수 있는 크롤러를 구현했습니다. 각 프로세싱 단계에 대해 디렉토리를 나누고 이를 모듈화 했습니다.

**2. 작업시간 단축**

- 이미 수집된 제품 데이터에 대해서 수집과정을 생략함으로써 작업시간을 단축했고, 실시간 편성정보와 가격 정보 수집 역시 unique키를 활용해 데이터베이스가 무거워지지 않도록 진행했습니다.
- 수집된 가격 정보를 직전 가격 정보와 비교하여 제품 정보 자체가 변경된 경우에만 제품 정보를 재수집하도록 처리하여 실시간 정보 변동에 대응할 수 있도록 처리했습니다.

**3. 정보 변경 대응**

- 쇼핑몰(콕) 데이터
    + 수집된 가격 정보를 직전 가격 정보와 비교하여 제품 정보 자체가 변경되거나 원가 정보가 변경된 경우에만 제품 정보를 재수집하도록 처리하여 실시간 정보 변동에 대응할 수 있도록 처리했습니다.
- 홈쇼핑 데이터 
    + 변경되는 방영 스케쥴에 대응하기 위해 수집된 데이터를 직접 처리하지 않고 당일 편성표에 대한 별도의 테이블을 만들어 실제 방영 스케쥴과 일치하도록 처리했습니다.


### 부족한점 :
**1. 개발 일정 관리**

- 데이터 관리에 대한 경험과 지식부족으로 예외 처리 작업에 많은 시간이 소요되었고, 그에 따라 최초 상정했던 ETL 구축 일정이 지연되었습니다. 이로 인해 최초 설계하였던 테이블 정의와 ERD에 변경점이 생겼으나 달라진 테이블 정의에 대한 문서화 및 공유에 대해서도 미흡한 면모를 보였습니다.

**2. DB 관리 미흡**

- ETL 구축 이후 운영 단계에서 우분투 환경 관리에 대한 경험 부족으로 DB 오류를 야기했고, 이에 대응하기 위한 시간이 소비됨으로 ETL 개발 일정이 다시 한 번 지연되었습니다.

**3. ODS 테이블 경량화 실패**
- 데이터를 수집하는 과정에서 최초 상정했던 수집 사항보다 수집 데이터가 적어지며 전처리 과정 이후 많은 데이터를 포기해야만 했습니다. 하지만 역량 부족으로 원천 데이터 크롤링 코드를 수정할 시간을 확보하지 못했고, 다소 ODS 테이블이 무거워지는 문제를 해결하지 못했습니다.

### 느낀점 :
**데이터 탐색 시 선정 기준**
- 사람마다, 그리고 기업마다의 데이터는 각자의 관리 방식, 내재된 인사이트 등 여러 이유로 서로 다른 형태를 지니고 있습니다. 내포한 내용이 좋고 그 형태 역시 깔끔한 데이터를 찾기란 쉽지 않다는 것을 느꼈고, 각각의 데이터를 하나의 형태로 통합하고 관리하는 것 역시 그러했습니다. 데이터 탐색 단계에서 수집하고 사용할 데이터를 선정하는 것은 **'사용하고 싶은'** 데이터가 아닌 **'사용할 수 있는'** 데이터에 조금 더 무게를 두어야 한다는 것, 아울러 이상치와 결측치에 대해서는 과감하게 대응할 수 있어야 한다는 것을 배울 수 있었습니다.